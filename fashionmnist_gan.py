# -*- coding: utf-8 -*-
"""FashionMNIST_GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nFEtdW5hFCd0KJvbYE4poJDQrS0kCEyp

## **Setup**

Importing packages, downloading and normalising the FashionMNIST dataset.
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import time

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()
plt.imshow(train_images[0])

train_images = train_images.reshape(train_images.shape[0],28,28, 1)
# normalise the data - shift pixel brightness representation into [-1 ; 1]
train_images = (train_images - 127.5) / 127.5
# check:
# train_images[0]

BATCH_SIZE = 100
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(train_images.shape[0]).batch(BATCH_SIZE)

"""## **Creating a Discriminator Model**"""

def make_discriminator_model():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Conv2D(7, (3, 3), padding="same", input_shape=(28, 28, 1)))
  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.LeakyReLU())
  model.add(tf.keras.layers.Dense(50, activation="relu"))
  model.add(tf.keras.layers.Dense(1))
  
  return model

discriminator = make_discriminator_model()

noise = np.random.rand(1,28,28,1).astype("float32")
plt.imshow(noise[0].reshape(28,28))
discriminator(noise)

discriminator_optimizer = tf.optimizers.Adam(1e-3)

def get_discriminator_loss(real_pred, fake_pred):
  real_pred = tf.sigmoid(real_pred)
  fake_pred = tf.sigmoid(fake_pred)
  # BCE between 1 values and each prediction about real images
  real_loss = tf.losses.binary_crossentropy(tf.ones_like(real_pred), real_pred)
  # BCE between 0 values and each pred. about fake images
  fake_loss = tf.losses.binary_crossentropy(tf.zeros_like(fake_pred), fake_pred)
  return real_loss + fake_loss

"""## **Creating a Generator**"""

def make_generator_model():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Dense(7*7*256, input_shape = (100, )))
  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Reshape((7,7,256)))
  model.add(tf.keras.layers.Conv2DTranspose(128,(3,3), padding="same"))
  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2DTranspose(64,(3,3), strides=(2,2), padding="same"))
  model.add(tf.keras.layers.BatchNormalization())
  model.add(tf.keras.layers.Conv2DTranspose(1,(3,3), strides=(2,2), padding="same"))
  
  return model

generator = make_generator_model()

generator_optimizer = tf.optimizers.Adam(1e-4)

def get_generator_loss(fake_pred):
  fake_pred = tf.sigmoid(fake_pred)
  # BCE between 1 values and each pred. about fake images
  # (Since the generator wants to fool the discriminator)
  fake_loss = tf.losses.binary_crossentropy(tf.ones_like(fake_pred), fake_pred)
  return fake_loss

"""## **Training**"""

def train(dataset, epochs):
  for _ in range(epochs):
    for images in dataset:
      images = tf.cast(images, tf.dtypes.float32)
      train_step(images)

def train_step(images):
  fake_image_noise = np.random.randn(BATCH_SIZE, 100).astype("float32")
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    generated_images = generator(fake_image_noise)
    real_output = discriminator(images)
    fake_output = discriminator(generated_images)

    gen_loss = get_generator_loss(fake_output)
    disc_loss = get_discriminator_loss(real_output, fake_output)

    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)
    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))

    print("generator loss:     ", np.mean(gen_loss))
    print("discriminator loss: ", np.mean(disc_loss))

train(train_dataset, 10)

plt.imshow(tf.reshape(generator(np.random.randn(1,100)), (28,28)), cmap="gray")